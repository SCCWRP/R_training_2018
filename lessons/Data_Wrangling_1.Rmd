
```{r setup, echo=FALSE, warning=FALSE, purl=FALSE, message=FALSE}
options(repos = "http://cran.rstudio.com/")
pkgs <- c("dplyr", "knitr")
x<-lapply(pkgs, library, character.only = TRUE)
opts_chunk$set(tidy = FALSE, message = F, warning = F)
```

* best practices - start with project creation data import
* more than one way to skin a cat - loops v apply v group/summarize
* More interaction with audience
* Less material to cover
* Use Bight example data
* Why would you manipulate, change data?
* Pipes v nesting v reassignment

# Data Wrangling Pt. 1

## Lesson Outline

* [Today's goals]
* [The tidyverse]
* [Data wrangling with `dplyr`]
* [Piping]
* [Joining data with `tidyr`]
* [Tidy data]

## Lesson Exercises

* [Exercise 1]

## Today's goals

Data wrangling (manipulation, ninjery, cleaning, etc.) is the part of any data analysis that will take the most time. While it may not necessarily be fun, it is foundational to all the work that follows. I often argue that mastering these skills can have more value than mastering a particular analysis.  

As before, we only have two hours to cover the basics of data wrangling. It’s an unrealistic expectation that you will be competent after this training. As such, the goals are to expose you to fundamentals and to develop an appreciation of what’s possible. I also want to provide resources that you can use for follow-up learning on your own.

Today you should be able to answer (or be able to find answers to) the following:

* Why do we need to manipulate data?
* What is the tidyverse? 
* What can you do with dplyr?
* What is piping?
* How are data joined?
* What is tidy data?  

## Exercise 1

As a refresher and to get our brains working, we're going to repeat the exercises from the our training last week.  The only exception is that we'll be using a monitoring dataset from the Bight.  In this exercise, we'll make a new project in RStudio and create a script for importing and working with these data today.

1. Start RStudio: To start both R and RStudio requires only firing up RStudio.  RStudio should be available from All Programs at the Start Menu.  Fire up RStudio. Take a minute or two to orient yourself to the different panes if you need a refresher from last time. 

2. Create a new project.  Name it "sccwrp_r_workshop".  We will use this for the rest of the workshop.

3. Create a new "R Script" in the Source (scripting) Pane, save that file into your project and name it "data_wrangling1.R". It'll just be a blank text file at this point.

4. Add in a comment line to separate this section.  It should look something like: `# Exercise 1: Scripting for data wrangling part 1`.

5. We need to get this project set up with some example data for our upcoming exercises.  You should have downloaded this already, but if not, download the Bight dataset from here: <https://github.com/SCCWRP/R_training_2018/raw/master/lessons/data/bight.xlsx>.  If not already created, make a folder in your new project named `data` and copy this file into this location.  

6. Add some lines to your script to import and store the data in your Enviroment. Remember that the data should be in your `data` folder in your project.  The `read_excel` function is your friend here and the path is `"data/bight.xlsx"`.  Remember to install/load the `readxl` library and to assign the data to a variable (`<-`)

7. Now run your script and make sure it doesn't throw any errors and you do in fact get a data frame (or tibble).

8. Explore the data frame using some of the functions we covered last time (e.g. `head()`,`dim()`, or `str()`).  This part does not need to be included in the script and can be done directly in the console. It is just a quick QA step to be sure the data read in as expected.

## The tidyverse

The [tidyverse](https://www.tidyverse.org/) is a set of packages that work in harmony because they share common data representations and design. The tidyverse package is designed to make it easy to install and load core packages from the tidyverse in a single command. With tidyverse, you'll be able to address all steps of data exploration.  

![](figure/data-science.png)

From the excellent book, [R for Data Science](http://r4ds.had.co.nz/), **data exploration** is the art of looking at your data, rapidly generating hypotheses, quickly testing them, then repeating again and again and again.  Tools in the tidyverse also have direct application to more formal analyses with many of the other available R packages on [CRAN](https://cran.r-project.org/).

You should already have the tidyverse installed, but let's give it a go if you haven't done this part yet:

```{r eval = F}
# install 
install.packages('tidyverse')
```

After installation, we can load the package:
```{r message = T}
# load
library(tidyverse)
```

Notice that the messages you get after loading are a bit different from other packages.  That's because tidyverse is a package that manages other packages.  Loading tidyverse will load all of the core packagbes:

-   [ggplot2](http://ggplot2.tidyverse.org), for data visualisation.
-   [dplyr](http://dplyr.tidyverse.org), for data manipulation.
-   [tidyr](http://tidyr.tidyverse.org), for data tidying.
-   [readr](http://readr.tidyverse.org), for data import.
-   [purrr](http://purrr.tidyverse.org), for functional programming.
-   [tibble](http://tibble.tidyverse.org), for tibbles, a modern re-imagining of data frames.

Other packages (e.g., `readxl`) are also included but they you will probably not use these as frequently.  

A nice freature of tidyverse is the ability to check for and install new versions of each package:

``` r
tidyverse_update()
#> The following packages are out of date:
#>  * broom (0.4.0 -> 0.4.1)
#>  * DBI   (0.4.1 -> 0.5)
#>  * Rcpp  (0.12.6 -> 0.12.7)
#> Update now?
#> 
#> 1: Yes
#> 2: No
```

As you'll soon learn using R, there are often several ways to achieve the same goal.  The tidyverse provides tools to address problems that can be solved with other packages or even functions from the base installation.  Tidyverse is admittedly an *opinionated* approach to data exploration, but it's popularity and rapid growth within the R community is a testament to the power of the tools that are provided. 

## Data wrangling with `dplyr`

![](figure/data-science-wrangle.png)

As the graphic implies, the data wrangling process includes data import, tidying, and transformation.  The process directly feeds into, and is not mutually exclusive, with the *understanding* or modelling side of data exploration.  More generally, I consider data wrangling as the manipulation or combination of datasets for the purpose of analysis.  

Wrangling begins with import and ends with an output of some kind, such as a plot or a model.  In a perfect world, the wrangling process is linear with no need for back-tracking.  In reality, we often uncover more information about a dataset, either through wrangling or modeling, that warrants re-evaluation or even gathering more data.  Data also come in many forms and the form you need for analysis is rarely the form of the input data.  For these reasons, data wrangling will consume most of your time in data exploration. 

All wrangling is based on a purpose.  No one wrangles for the sake of wrangling (usually), so the process always begins by answering the following two questions:

* What do my input data look like?
* What should my input data look given what I want to do?

At the most basic level, going from what your data looks like to what it should look like will require a few key operations.  Some common examples:

* Selecting specific variables
* Filtering observations by some criteria
* Adding or modifying existing variables
* Renaming variables
* Arranging rows by a variable
* etc.

The `dplyr` package tries to provide easy tools for these common data manipulation tasks. It is built to work directly with data frames and this is one of the foundational packages in what is now known as the tidyverse. The philosophy if dplyr is that one function does one thing and the name of the function says what it does. This is where the tidyverse philosophy departs from other packages and even base R.  It is meant to be easy, logical, and intuitive.  There is a lot of great info on dplyr. If you have an interest, I’d encourage you to look more. The vignettes are particularly good.

* [`dplyr` GitHub repo](https://github.com/hadley/dplyr)
* [CRAN page: vignettes here](http://cran.rstudio.com/web/packages/dplyr/)

Let's begin using dplyr.

## Piping

## Joining data with `tidyr`

## Tidy data

## Attribution

Content in this lesson was pillaged extensively from the USGS-R training curriculum [here](https://github.com/USGS-R/training-curriculum) and [R for data Science](https://github.com/hadley/r4ds).