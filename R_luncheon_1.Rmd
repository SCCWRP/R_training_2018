
```{r setup, echo=FALSE, warning=FALSE, purl=FALSE, message=FALSE}
options(repos = "http://cran.rstudio.com/")
pkgs <- c("dplyr", "knitr")
x<-lapply(pkgs, library, character.only = TRUE)
opts_chunk$set(tidy = FALSE, message = F, warning = F)
```

# R luncheon 1

## Lesson Outline

* [Problem scope]
* [Housekeeping]
* [Data import]
* [Data wrangle]
* [Plot the data]
* [Summary]

Welcome to the first R luncheon!  These short and informal sessions are designed for continued learning of R that builds off of our previous training sessions (access any of the earlier content from our [home page](index.html)).  Our goal is to understand how we use R to approach real-world examples in bite size chunks. Each session will focus on a selected problem that may be specific to a research area but contains analysis challenges that are shared no matter the context.  The discussion will contain two parts:

1. Presentation of the problem: what we are starting with and where we want to go. 

1. Steps to address the problem in general terms

1. Specific steps to address the problem using R

1. Execution of the workflow to achieve the final product.  

Please note that this is not an introduction to R. You are expected to have some background knowledge with the understanding that you may not be comfortable using R to complete all steps of an analysis. You should leave the session with a greater understanding of how R can help you develop an efficient and repeatable workflow for your own data challenges.  I encourage lively discussion as we go through the workflow for each luncheon, so ask plenty of questions!

# Problem scope

Today's session will use a compiled dataset that describes filtration of stormwater at different treatment locations in California. The dataset has measured influent and effluent concentrations of different water quality parameters that were measured during storm events.  The general goal is to quantify filtration effectiveness by comparing influent/effluent concentrations with the storm events across several locations. This will provide a benchmark of how well the filters are removing potentially harmful water quality constituents during high flow events.  

![](figure/rawdat.JPG)

We want to create a plot like this after getting the raw data in the correct format. 

![](figure/finalplo.JPG)

The data are messy.  The spreadsheet includes measurements from different locations where record-keeping was not consistent.  Each location may also include several filtration samples, with measurements for one or more storm events.  There are several columns in the dataset, not all of which are relevant for our analysis.  Here's a screenshot:

![](figure/lunchdatex.PNG)

We want to import the data, wrangle the information to a consistent format, and plot the data to help us assess how well the filtration systems are removing water quality constituents. Each of these steps can be performed in R.

1. Import the Excel spreadsheet

1. Wrangle the data to a consistent format. This will require removing extra columns we don't need, filtering the observations (rows) for a water quality parameter we're interested in, and develop a consistent convention that will allow to compare between stormwater treatment locations, filtration samples, and storm events.

1. Plot the data using a simple scatterplot of effluent concentration (y-axis) against influent concentration (x-axis).  Our hope is that effluent concentrations will be lower than influent concentrations as a measure of how well the filtration systems are working.

# Housekeeping

Let's start by opening RStudio, creating a new project, and downloading the data to the project.  

## Open RStudio

Find the RStudio shortcut and fire it up.  You should see something like this: 

![](figure/rstudio.png)

## Create a new project in RStudio

To create a new project, click on the File menu at the top and select 'New project...'

![](figure/rstudio_proj.jpg)

## Download the data to your project

You can download the data from this [link](https://sccwrp.github.io/SCCWRP_R_training/data/CA_DetentionBasin2.xlsx).  Once downloaded, you'll have to copy and paste the Excel file to your project.  Run `getwd()` in the R console if you're not sure where this is. 

You can also download the data file directly in R. This will download the data and save the file to your new RStudio project in a single step.

```{r, eval = F}
# download the data
download.file(
  url = 'https://sccwrp.github.io/SCCWRP_R_training/data/CA_DetentionBasin2.xlsx',
  destfile= 'CA_DetentionBasin2.xlsx'
  )
```

# Data import

Now that we've got our project setup with the data, let's open a new script and load some R packages that we'll be using. 

Open a new script from the File menu...

![](figure/rstudio_script.jpg)

Once the script is open, save it using the drop down file menu on the top left.  Give it an informative name (e.g.,`Rluncheon1.R`) and save it in your project's home directory.  This should be the default location selected by RStudio when you save the file.

We'll be using functions from the [tidyverse](https://www.tidyverse.org/) collection of packages to import, wrangle, and plot the data.  Checkout our training material [here](https://sccwrp.github.io/SCCWRP_R_training/Data_Wrangling_1.html#the-tidyverse) if you need a brush up.  Run this line in the R console if you don't have the tidyverse installed.

```{r, eval = F}
install.packages('tidyverse')
```

In the script you just opened, add the following lines to load the tidyverse package and the readxl package (for data import).

```{r, message = F, warning = F}
library(tidyverse)
library(readxl)
```

Then add this line to import the dataset using the `read_excel()` function.  The imported dataset will be assigned to the `dat` variable in the workspace for your current R session.

```{r, echo = F, message = F}
dat <- read_excel('data/CA_DetentionBasin2.xlsx')
```
```{r, eval = F}
dat <- read_excel('CA_DetentionBasin2.xlsx')
```

Let's get a feeling for the dataset before we proceed.

```{r}
head(dat)
str(dat)
```

# Data wrangle

There's plenty of information in this dataset that we don't need.  Let's pare it down so it's more manageable for our question.  Before we do that, we need to decide which columns are important and which water quality parameter we care about.  

As mentioned before, we want to compare influent and effluent concentrations measured for one or more samples, for one or more storms, at different stormwater treatment locations.  The columns we care about are:

* `SITENAME`: unique identifier for the stormwater treatment location
* `MSNAME`: identifier for different samples from a stormwater treatment location, this is where the influent and effluent measurements are identified
* `Storm #`: identifier for storm events that occurred at a stormwater treatment location
* `WQX Parameter`: name of measured water quality values
* `WQ Analysis Value`: concentration of the measured water quality values, as influent or effluent

We can use the `select()` function to pull the columns we want.  Here we are using pipes to make the syntax a little more clear.  Checkout the content [here](https://sccwrp.github.io/SCCWRP_R_training/Data_Wrangling_1.html#selecting) for background on the `select()` function and [here](https://sccwrp.github.io/SCCWRP_R_training/Data_Wrangling_1.html#piping) for a refresher on pipes.    

```{r}
dat <- dat %>%
  select(SITENAME, MSNAME, `Storm #`,`WQX Parameter`, `WQ Analysis Value`)
```

We just created a new `dat` object by overwriting the previous one we made when we imported the raw data.  The new object contains only the columns we care about.  Also note that some column names had to be surrounded by backticks (``).  R doesnt' like it when names have "special characters", such as spaces, so sometimes we need to use the backticks when calling the name. 

Now that we have the columns we want, we still need to get the observations for the water quality parameter we want to analyze. Let's see what's available:

```{r}
unique(dat$`WQX Parameter`)
```

Let's pull out the rows for lead.  We can use the `filter()` function to get these rows by "filtering" the observations where `WQX Parameter` is equal to `Lead`.  See [here](https://sccwrp.github.io/SCCWRP_R_training/Data_Wrangling_1.html#filtering) for a refresher of filter.

```{r}
dat <- dat %>% 
  filter(`WQX Parameter` == 'Lead')
```

Let's examine our handywork:

```{r}
str(dat)
head(dat)
```

Much more manageable!  Now comes the hard part... we need to get the data in a format where we can easily compare the influent and effluent concentrations, while keeping the data structured in a way so that samples for each storm event and each treatment location are consistently referenced in the rows. This is information in the dataset that needs to be retained to meaningfully evaluate filtration effectiveness.  We can't easily compare the influent/effluent concentrations in the current data because they are in the same column!

Here's an example of what the data should look like for us to plot effluent against influent:
```{r echo = F}
nobs <- 6
datex <- tibble(
  SITENAME = rep(c('site1', 'site2'), each = nobs/2), 
  MSNAME = rep(c('sample1', 'sample2', 'sample3'), times = nobs/3), 
  `Storm #` = sample(c(1, 2), nobs, replace = T), 
  influent = runif(nobs, 6, 10), 
  effluent = runif(nobs, 3, 7)
) %>% 
  arrange(SITENAME, MSNAME, `Storm #`)
datex
```
There are three things about this dataset that differ from our actual data:

1) Consistent naming convention between sites and samples

1) Separate columns for influent and effluent

1) Each row references the site name, sample name, and storm number correctly for the influent and effluent observations

Before we proceed, we'll make life a little easier by taking care of the storms column.  We would retain these columns if this were a real analysis, but this will get us to lunch quicker... so, we can use some tools from the dplyr package (as part of the tidyverse) to help us out.  We will use `group_by()` and `summarise()` to average the water quality concentrations across storms for the unique site/sample combinations (see [here](https://sccwrp.github.io/SCCWRP_R_training/Data_Wrangling_2.html#group_by_and_summarize) for more info).

```{r}
dat <- dat %>% 
  group_by(SITENAME, MSNAME) %>% 
  summarise(avewq = mean(`WQ Analysis Value`))
str(dat)
head(dat)
```

We are almost there.  Now we just need to get the influent and effluent values in different columns.  If you recall our earlier training sessions [here](https://sccwrp.github.io/SCCWRP_R_training/Data_Wrangling_2.html#spreading), we can use the `spread()` function from the tidyr package (again, included in the tidyverse).  This function lets us "spread" values in one column to multiple columns using "key" values in a second column. The key values are used as the column names for the values that are spread from one to many columns.   

Here's an example of spreading with a different dataset:

![](figure/tidy-8.png)

Let's try this on our dataset, where the key column is `MSNAME` and the value column is `avewq` from our summarised data.

```{r}
dat %>%
  spread(MSNAME, avewq)
```

So this isn't exactly right. The data are definitely spread but we have unique columns for every influent/effluent sample in our dataset.  We'll need to separate the sample names and the influent/effluent designations in the `MSNAME` column before we can proceed.  If you recall from our past training, these data are not ["tidy"](https://sccwrp.github.io/SCCWRP_R_training/Data_Wrangling_2.html#tidy_data), meaning, among other things, that the `MSNAME` column contains two pieces of information in one cell.  The column includes information on both the sample name and the influent/effluent designation. 

There's a handy function from base R called `gsub()` that lets you find patterns in a character string and replace them with values of your choosing. Here, we want to create two new columns from `MSNAME`, one with only the sample name (numbers) and another with only the influent/effluent designations (letters).  This will allow the `spread()` function to create a single column for influent and a single column for effluent, while retaining the correct sample and site names in the rows. This can be done as follows:

```{r}
dat <- dat %>% 
  mutate(
    samplename = gsub('[aA-zZ]+', '', MSNAME),
    infeff = gsub('[0-9]+', '', MSNAME)
  )
dat
```

We just used the `mutate()` function to create two new columns, where `samplename` was created by finding all letters in `MSNAME` and removing them and `infeff` was created by finding all digits in `MSNAME` and removing them. The `gsub()` function is very powerful and uses "regular expression matching" to search for patterns in the character strings that can be replaced (in this case, with nothing `''`).  This could easily cover a few hours of instruction, but here's a [cheatsheet](https://www.rstudio.com/wp-content/uploads/2016/09/RegExCheatsheet.pdf) on regular expressions if you want to learn on your own. 

Now all we have to do is remove the old `MSNAME` column and spread as before.
```{r}
dat <- dat %>% 
  select(-MSNAME) %>% 
  spread(infeff, avewq)
dat
```

# Plot the data

Now we're ready to plot!  Let's use ggplot (refresher [here](https://sccwrp.github.io/SCCWRP_R_training/Viz_and_Graphics.html#ggplot2) to make a simple scatterplot with a trend line.
```{r}
ggplot(dat, aes(x = I, y = E)) +
  geom_point() + 
  stat_smooth(method = 'lm', se = F)
```

Let's make the comparisons a little easier to see.  We'll add a line through the origin and use the same scale for both axes.
```{r}
ggplot(dat, aes(x = I, y = E)) +
  geom_point() + 
  stat_smooth(method = 'lm', se = F) + 
  geom_abline(intercept = 0, slope = 1, linetype = 'dashed') + 
  scale_y_continuous(limits = c(0, 70)) +
  scale_x_continuous(limits = c(0, 70)) + 
  coord_equal()
```

# Summary

Let's recap the steps:

1. Imported the Excel spreadsheet using the `read_excel()` function from the readxl package (in tidyverse)

1. Wrangled the data to a consistent format.
     1. Selected the columns we wanted using `select()`
     1. Filtered the rows for lead using `filter()`
     1. Averaged concenctrations across storm events using `group_by()` and `summarise()`
     1. Created unique columns for sample name and influent/effluent categories using `mutate()` and `gsub()`
     1. Spread the data using `spread()` to get the influent/effluent concentrations in differnt columns

1. Compared the effluent and influent concentrations using ggplot

Our conclusion?  Looks like the filters are doing a good job removing lead from the stormwater.  
